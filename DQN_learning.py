"""
File usage:
- Update hash function and config variables
- To train: python DQN_learning.py train
- To evaluate with GUI: python DQN_learning.py gui
- To evaluate without GUI: python DQN_learning.py 

Note: Github Copilot auto-generated segments of code within this file.
"""
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
from tqdm import tqdm
from snake_gui import *
import matplotlib.pyplot as plt

train_flag = 'train' in sys.argv
gui_flag = 'gui' in sys.argv

"""
fullboard
hybrid
"""
ACTIVE_HASH_FUNCTION = 'hybrid' 

# Config variables
num_episodes = 250000
batch_size = 128
gamma = 0.9999
learning_rate = 0.0005
buffer_capacity = 100000
epsilon_decay = 0.999

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

setup(GUI=gui_flag)
env = game  # Gym environment already initialized within snake_gui.py

# ============================================================================
# NN ARCHITECTURE
# ============================================================================
class DQN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        return self.fc4(x)

# ============================================================================
# EXPERIENCE REPLAY BUFFER
# ============================================================================
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)
    
    def __len__(self):
        return len(self.buffer)

# ============================================================================
# HASH FUNCTIONS
# ============================================================================
def hash_whole_board_all_info():
    """Hashes full board observation
    
    Returns flattened grid plus direction information:
    - Full grid: grid_size * grid_size values (0-5 for empty/body/foods/head)
    - Direction: 2 values (x, y)
    """
    obs = env.get_observation()
    direction_features = np.array([env.direction[0], env.direction[1]], dtype=np.float32)
    state_features = np.concatenate([obs.flatten(), direction_features])
    
    return torch.FloatTensor(state_features).to(device)

def hash_hybrid():
    """
    Includes the following:
    1. Head position: 2 features
    2. Direction: 2 features  
    3. Food positions and distances: 9 features
    4. Environment detection: 8 features
        - Distance to wall/body & snake body presence in 4 cardinal directions
    5. Body occupancy grid: 16 features
       - Divides board into 4x4 regions, marks which contain body
    6. Snake length: 1 feature
        - Goal for including this - if snake is longer, maybe it will decide to play more caustiously
    7. Immediate surroundings: 4 features
        - For each direction, 1 if we can move without dying, 0 if not.
        - Including this because the snake likes to run into itself. 
    """
    head_x, head_y = env.snake[0]
    board_size = env.grid_size
    
    # 1. Snake Head
    head_x_norm = head_x / board_size
    head_y_norm = head_y / board_size
    
    # 2. Direction
    dir_x, dir_y = env.direction
    
    # 3. Food positions
    food_features = []
    for fruit_type in sorted(env.food_positions.keys()):
        x, y = env.food_positions[fruit_type]
        food_features.append(x / board_size)
        food_features.append(y / board_size)
        # L1 norm
        dist = (abs(x - head_x) + abs(y - head_y)) / (board_size * 2)
        food_features.append(dist)
    
    # 4. Environment detection
    # Detect distance to wall/body and body presence in 4 cardinal directions
    # This section was generated by Github Copilot
    # Prompt: In snake game, for each direction (N,E,S,W), compute distance to nearest wall or body segment,
    # and a binary flag indicating if a body segment is present in that direction.
    danger = []
    for dx, dy in [(0, -1), (1, 0), (0, 1), (-1, 0)]:
        distance = 0
        current_x, current_y = head_x, head_y
        body_found = False
        while True:
            current_x += dx
            current_y += dy
            distance += 1
            
            # Check if we hit a wall
            if current_x < 0 or current_x >= board_size or current_y < 0 or current_y >= board_size:
                break
            
            # Check if we hit snake body
            if (current_x, current_y) in env.snake[1:]:
                body_found = True
                break
        
        # Normalize distance by board size
        normalized_distance = distance / board_size
        danger.append(normalized_distance)
        danger.append(1.0 if body_found else 0.0)  # Binary flag for body presence
    
    # 5. Body occupancy grid
    # This section was also generated by Github Copilot
    # Prompt: In snake game, divide board into 4x4 grid regions, for each region,
    # mark 1 if any body segment is present, else 0.
    grid_divisions = 4
    region_size = board_size / grid_divisions
    body_grid = []
    
    for gy in range(grid_divisions):
        for gx in range(grid_divisions):
            # Check if any body segment is in this region
            region_has_body = 0.0
            for bx, by in env.snake[1:]:  # All body segments (not head)
                if (gx * region_size <= bx < (gx + 1) * region_size and
                    gy * region_size <= by < (gy + 1) * region_size):
                    region_has_body = 1.0
                    break
            body_grid.append(region_has_body)
    
    # 6. Snake length
    snake_length = len(env.snake) / (board_size * board_size)
    
    # 7. Immediate surroundings:
    action_safety = []
    for dx, dy in [(0, -1), (1, 0), (0, 1), (-1, 0)]:
        next_x, next_y = head_x + dx, head_y + dy
        is_safe = 1.0
        if next_x < 0 or next_x >= board_size or next_y < 0 or next_y >= board_size:
            is_safe = 0.0
        elif (next_x, next_y) in env.snake[1:]:
            is_safe = 0.0 
        
        action_safety.append(is_safe)
    
    state = ([head_x_norm, head_y_norm, dir_x, dir_y] + 
             food_features + danger + body_grid + 
             [snake_length] + action_safety)
    
    return torch.FloatTensor(state).to(device)

HASH_FUNCTIONS = {
    'fullboard': hash_whole_board_all_info,
    'hybrid': hash_hybrid,
}

hash_state = HASH_FUNCTIONS[ACTIVE_HASH_FUNCTION]

print(f"Using hash function: {ACTIVE_HASH_FUNCTION}")

# ============================================================================
# DQN TRAINING
# ============================================================================
def dqn(num_episodes=10000, batch_size=64, gamma=0.99, 
              learning_rate=0.001, buffer_capacity=50000,
              epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,
              target_update_freq=100):
    """
    Run DQN-learning algorithm for a specified number of episodes.
    
    Parameters:
    - num_episodes: Number of episodes to run.
    - batch_size: Batch size
    - gamma: Discount factor
    - learning_rate: Learning rate
    - buffer_capacity: Maximum size of replay buffer
    - epsilon_start: Initial exploration rate
    - epsilon_end: Minimum exploration rate
    - epsilon_decay: Decays epsilon per episode
    - target_update_freq: How often to update target network
    
    Returns:
    - policy_net: Trained policy network
    - episode_rewards: List of total rewards per episode
    """
    sample_state = hash_state()
    
    input_size = len(sample_state)
    hidden_size = 256
    output_size = env.action_space.n
    
    policy_net = DQN(input_size, hidden_size, output_size).to(device)
    target_net = DQN(input_size, hidden_size, output_size).to(device)
    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()  # Target network must always be in eval mode
    
    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)
    replay_buffer = ReplayBuffer(buffer_capacity)
    
    epsilon = epsilon_start
    episode_rewards = []
    episode_lengths = []
    losses = []
    
    for episode in tqdm(range(num_episodes)):
        env.reset()
        state = hash_state()
        total_reward = 0
        steps = 0
        
        while True:
            if np.random.rand() < epsilon:
                # random(a∈A); with probability P = ε
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    # arg max_a∈A Qopt(s,a); with probability P = (1 − ε)
                    q_values = policy_net(state)
                    action = q_values.argmax().item()
            
            obs, reward, done, info = env.step(action)

            if done and reward <= 0:
                reward_adjusted = -1000  # Strong penalty for dying
            else:
                reward_adjusted = reward - 2  # Penalize each step to encourage efficiency
            
            next_state = hash_state()
            total_reward += reward_adjusted
            steps += 1
            
            replay_buffer.push(state, action, reward_adjusted, next_state, done)
            state = next_state
            
            if len(replay_buffer) >= batch_size:
                batch = replay_buffer.sample(batch_size)
                states, actions, rewards, next_states, dones = zip(*batch)
                
                states_batch = torch.stack(states)
                actions_batch = torch.LongTensor(actions).to(device)
                rewards_batch = torch.FloatTensor(rewards).to(device)
                next_states_batch = torch.stack(next_states)
                dones_batch = torch.FloatTensor(dones).to(device)
                
                # Compute current Q-values: Q(s,a)
                current_q_values = policy_net(states_batch).gather(1, actions_batch.unsqueeze(1))
                
                # Compute target Q-values: r + gamma * max_a' Q_target(s', a')
                with torch.no_grad():
                    next_q_values = target_net(next_states_batch).max(1)[0]
                    target_q_values = rewards_batch + gamma * next_q_values * (1 - dones_batch)
                
                loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
                
                optimizer.zero_grad()
                loss.backward()
                # Gradient clipping to prevent exploding gradients
                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)
                optimizer.step()
                
                losses.append(loss.item())
            
            if done:
                break
        
        if episode % target_update_freq == 0:
            target_net.load_state_dict(policy_net.state_dict())
        
        epsilon = max(epsilon_end, epsilon * epsilon_decay)
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
    
    return policy_net, episode_rewards

# ============================================================================
# TRAINING & EVALUATION
# ============================================================================
if train_flag:
    print(f"Episodes: {num_episodes}")
    print(f"Batch size: {batch_size}")
    print(f"Gamma: {gamma}")
    print(f"Learning rate: {learning_rate}")
    print(f"Buffer capacity: {buffer_capacity}")
    print(f"Epsilon decay: {epsilon_decay}")
    
    policy_net, episode_rewards = dqn(
        num_episodes=num_episodes,
        batch_size=batch_size,
        gamma=gamma,
        learning_rate=learning_rate,
        buffer_capacity=buffer_capacity,
        epsilon_decay=epsilon_decay
    )
    
    file_name = f'models/dqn/dqn_model_{num_episodes}_{learning_rate}_{ACTIVE_HASH_FUNCTION}.pth'
    torch.save(policy_net.state_dict(), file_name)
    print(f"Model saved to {file_name}")
else:
    file_name = f'models/dqn/dqn_model_{num_episodes}_{learning_rate}_{ACTIVE_HASH_FUNCTION}.pth'
    print(f"Loading {file_name}")
    
    checkpoint = torch.load(file_name)
    
    sample_state = hash_state()
    input_size = len(sample_state)
    hidden_size = 256
    output_size = env.action_space.n
    
    policy_net = DQN(input_size, hidden_size, output_size).to(device)
    
    # Handle both checkpoint format and direct state_dict format
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        policy_net.load_state_dict(checkpoint['model_state_dict'])
    else:
        policy_net.load_state_dict(checkpoint)
    
    print(f"Model loaded successfully")
    
    num_eval_episodes = 1000
    max_steps_per_episode = 50000 # Avoid looping
    
    policy_net.eval()
    episode_rewards = []
    episode_lengths = []
    episodes_cut_short = 0
    
    print(f"\nEvaluating DQN for {num_eval_episodes} episodes...")
    print(f"Max steps per episode: {max_steps_per_episode}")
    
    eval_start_time = time.time()
    
    for episode in tqdm(range(num_eval_episodes)):
        env.reset()
        if gui_flag:
            from snake_gui import reset_game_state
            reset_game_state()
        state = hash_state()
        total_reward = 0
        steps = 0
        
        while True: 
            # Softmax action selection for evaluation
            with torch.no_grad():
                q_values = policy_net(state)
                action_probs = torch.softmax(q_values / 0.1, dim=0)
                action = torch.multinomial(action_probs, 1).item()
            
            obs, reward, done, info = env.step(action)
            total_reward += reward
            steps += 1
            
            state = hash_state()
            
            if gui_flag:
                refresh(action, reward, done, info, delay=0.05)
            
            if steps >= max_steps_per_episode:
                episodes_cut_short += 1
                episode_rewards.append(total_reward)
                break
            
            if done:
                break
        
        if steps < max_steps_per_episode:
            episode_rewards.append(total_reward)
            episode_lengths.append(steps)
    
    eval_time = time.time() - eval_start_time
    

    print(f"Average reward over {num_eval_episodes} evaluation episodes: {np.mean(episode_rewards):.4f}")
    if len(episode_lengths) > 0:
        print(f"Average episode length (number of actions, excluding loops): {np.mean(episode_lengths):.4f}")
    else:
        print(f"Average episode length (number of actions, excluding loops): N/A (all episodes looped)")
    print(f"Episodes cut short due to max steps: {episodes_cut_short} ({100*episodes_cut_short/num_eval_episodes:.2f}%)")
    print(f"Total time for {num_eval_episodes} evaluation episodes: {eval_time:.4f} seconds")
    print(f"Min reward: {np.min(episode_rewards):.4f}")
    print(f"Max reward: {np.max(episode_rewards):.4f}")